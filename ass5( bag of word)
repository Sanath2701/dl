 import numpy as np
 import tensorflow as tf
 from tensorflow.keras.preprocessing.text import Tokenizer
 from tensorflow.keras.preprocessing.sequence import skipgrams
 from tensorflow.keras.models import Sequential
 from tensorflow.keras.layers import Embedding, Dense, Lambda
 import keras.backend as K
 # Sample corpus
 corpus = [
    "Machine learning is fascinating",
    "Deep learning drives artificial intelligence",
    "Natural language processing is part of AI",
    "Neural networks are powerful"
 ]
 # Tokenize the corpus
 tokenizer = Tokenizer()
 tokenizer.fit_on_texts(corpus)
 word2id = tokenizer.word_index
 id2word = {v: k for k, v in word2id.items()}
 vocab_size = len(word2id) + 1  # +1 for padding (if any)
 print("Vocabulary Size:", vocab_size)
 print("Word Index Mapping:", word2id)

 # Convert sentences to sequences of integers
 sequences = tokenizer.texts_to_sequences(corpus)
 print("\nTokenized Sequences:", sequences)

 # Generate skip-gram pairs
 window_size = 2
 pairs = []
 for sequence in sequences:
    for center_index, word_id in enumerate(sequence):
        # Define window boundaries
        start = max(0, center_index - window_size)
        end = min(len(sequence), center_index + window_size + 1)
        context_words = [sequence[i] for i in range(start, end) if i !
 = center_index]
        for context_word in context_words:
            pairs.append((context_word, word_id))  # (context, target)
 # Split into X (context) and y (target)
 X_train = np.array([x for x, _ in pairs])
 y_train = np.array([y for _, y in pairs])
 print("\nSample Training Pairs (context → target):")
 for i in range(5):
    print(f"{id2word[X_train[i]]} → {id2word[y_train[i]]}")

 # Define parameters
 embedding_dim = 8
 # Define model architecture properly
 model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=embedding_dim, 
input_length=1),
    # Use tf.reduce_mean instead of K.mean for safety
    Lambda(lambda x: tf.reduce_mean(x, axis=1), 
output_shape=(embedding_dim,)),
    Dense(vocab_size, activation='softmax')
 ])
 # Build model explicitly with input shape
 model.build(input_shape=(None, 1))
 # Compile model
 model.compile(loss='sparse_categorical_crossentropy', 
optimizer='adam')
 # Print summary
 model.summary()
 # Train the model
 history = model.fit(X_train, y_train, epochs=200, verbose=0)
 print("\n✅ Model training complete!")

# Get learned word embeddings
 embeddings = model.layers[0].get_weights()[0]
 print("\nWord Embedding Shape:", embeddings.shape)
 # Display embeddings for each word
 for word, idx in word2id.items():
    print(f"{word}: {embeddings[idx]}")
 # Find most similar words (cosine similarity)
 from sklearn.metrics.pairwise import cosine_similarity
 def find_similar(word):
    if word not in word2id:
        return "Word not in vocabulary."
    word_vec = embeddings[word2id[word]].reshape(1, -1)
    similarities = cosine_similarity(word_vec, embeddings)
    similar_indices = similarities.argsort()[0][-5:][::-1]
    print(f"\nTop similar words to '{word}':")
    for idx in similar_indices:
        print(f"{id2word.get(idx, '')}  (similarity: {similarities[0]
 [idx]:.3f})")
# Try with some examples
 find_similar("learning")
 find_similar("neural")
