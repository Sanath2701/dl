import numpy as np 
 
# --- 1. Activation Functions --- 
def sigmoid(x): 
    return 1 / (1 + np.exp(-x)) 
 
def sigmoid_derivative(x): 
    return x * (1 - x)

class SimpleRNN: 
    def __init__(self, input_size, hidden_size, output_size, 
learning_rate=0.01): 
        self.hidden_size = hidden_size 
        self.learning_rate = learning_rate 
         
        # Weights 
        self.W_xh = np.random.randn(input_size, hidden_size) * 0.01 
        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01 
        self.W_ho = np.random.randn(hidden_size, output_size) * 0.01 
         
        # Biases 
        self.b_h = np.zeros((1, hidden_size)) 
        self.b_o = np.zeros((1, output_size)) 
     
    # Forward pass 
    def forward(self, inputs): 
        self.hidden_states = {} 
        self.outputs = {} 
        h_prev = np.zeros((1, self.hidden_size)) 
         
        for t, x in enumerate(inputs): 
            x = x.reshape(1, -1) 
            h_prev = sigmoid(np.dot(x, self.W_xh) + np.dot(h_prev, 
self.W_hh) + self.b_h) 
            self.hidden_states[t] = h_prev 
            output = np.dot(h_prev, self.W_ho) + self.b_o 
            self.outputs[t] = output 
         
        return self.outputs[len(inputs) - 1] 
     
    # Backward pass (BPTT) 
    def backward(self, inputs, targets): 
        dW_xh = np.zeros_like(self.W_xh) 
        dW_hh = np.zeros_like(self.W_hh) 
        dW_ho = np.zeros_like(self.W_ho) 
        db_h = np.zeros_like(self.b_h) 
        db_o = np.zeros_like(self.b_o) 
        dh_next = np.zeros((1, self.hidden_size)) 
         
        for t in reversed(range(len(inputs))): 
            x = inputs[t].reshape(1, -1) 
            h_t = self.hidden_states[t] 
            h_prev_t = self.hidden_states[t-1] if t > 0 else np.zeros_like(h_t) 
             
            # Output gradient 
            d_output = self.outputs[t] - targets[t].reshape(1, -1) 
            dW_ho += np.dot(h_t.T, d_output) 
            db_o += d_output 
             
            # Hidden state gradient 
            dh = np.dot(d_output, self.W_ho.T) + dh_next 
            dh_raw = dh * sigmoid_derivative(h_t) 
            dW_xh += np.dot(x.T, dh_raw) 
            dW_hh += np.dot(h_prev_t.T, dh_raw) 
            db_h += dh_raw 
             
            dh_next = np.dot(dh_raw, self.W_hh.T) 
         
        # Update weights and biases 
        self.W_xh -= self.learning_rate * dW_xh 
        self.W_hh -= self.learning_rate * dW_hh 
        self.W_ho -= self.learning_rate * dW_ho 
        self.b_h -= self.learning_rate * db_h 
        self.b_o -= self.learning_rate * db_o 
 
# --- 3. Example Usage --- 
# Dummy sequence data 
input_sequence = np.array([[0.1], [0.2], [0.3], [0.4], [0.5]]) 
target_sequence = np.array([[0.2], [0.4], [0.6], [0.8], [1.0]]) 
 
# Initialize RNN 
input_size = 1 
hidden_size = 5 
output_size = 1 
rnn = SimpleRNN(input_size, hidden_size, output_size, 
learning_rate=0.05) 
 
# Training loop 
epochs = 1000 
print("Training Simple RNN...") 
for epoch in range(epochs): 
    # Forward pass 
    predicted_output = rnn.forward(input_sequence) 
     
    # Loss (MSE only on final output) 
    loss = np.mean((predicted_output - target_sequence[-1])**2) 
     
    # Backward pass 
    rnn.backward(input_sequence, target_sequence) 
     
    if (epoch + 1) % 100 == 0: 
        print(f"Epoch {epoch+1}, Loss: {loss:.4f}") 
 
# Test the trained RNN 
print("\nTesting the trained RNN:") 
test_input_sequence = np.array([[0.6], [0.7], [0.8], [0.9], [1.0]]) 
final_prediction = rnn.forward(test_input_sequence) 
print(f"Input Sequence: {test_input_sequence.flatten()}") 
print(f"Predicted Final Output: {final_prediction.flatten()}")
