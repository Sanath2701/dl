import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix

# =========================
# Load and Explore Dataset
# =========================
dataset = pd.read_csv("creditcard.csv")

print("Columns:", list(dataset.columns))
print(dataset.describe())
print("Any nulls? ", dataset.isnull().values.any())
print("Unique labels: ", dataset['Class'].unique())
print("Class distribution:\n", pd.value_counts(dataset['Class'], sort=True))

# Class distribution plot
count_classes = pd.value_counts(dataset['Class'], sort=True)
count_classes.plot(kind='bar', rot=0)
plt.xticks([0, 1], ["Normal", "Fraud"])
plt.title("Transaction Distribution")
plt.xlabel("Class")
plt.ylabel("Number of Observations")
plt.show()

# =========================
# Preprocessing
# =========================
sc = StandardScaler()
dataset['Time'] = sc.fit_transform(dataset['Time'].values.reshape(-1, 1))
dataset['Amount'] = sc.fit_transform(dataset['Amount'].values.reshape(-1, 1))

raw_data = dataset.values
labels = raw_data[:, -1]   # last column = target
data = raw_data[:, 0:-1]   # features

# Split into train & test sets
train_data, test_data, train_labels, test_labels = train_test_split(
    data, labels, test_size=0.2, random_state=2021
)

# Normalize data between 0â€“1
min_val = tf.reduce_min(train_data)
max_val = tf.reduce_max(train_data)
train_data = (train_data - min_val) / (max_val - min_val)
test_data = (test_data - min_val) / (max_val - min_val)
train_data, test_data = tf.cast(train_data, tf.float32), tf.cast(test_data, tf.float32)

train_labels = train_labels.astype(bool)
test_labels = test_labels.astype(bool)

normal_train_data = train_data[~train_labels]
fraud_train_data = train_data[train_labels]
normal_test_data = test_data[~test_labels]
fraud_test_data = test_data[test_labels]

print("Normal Train:", len(normal_train_data))
print("Fraud Train:", len(fraud_train_data))
print("Normal Test:", len(normal_test_data))
print("Fraud Test:", len(fraud_test_data))

# =========================
# Autoencoder Architecture
# =========================
input_dim = normal_train_data.shape[1]  # number of features (30)
encoding_dim = 14
hidden_dim_1 = int(encoding_dim / 2)   # 7
hidden_dim_2 = 4

# Input layer
input_layer = tf.keras.layers.Input(shape=(input_dim,))

# Encoder
encoder = tf.keras.layers.Dense(
    encoding_dim, activation="tanh",
    activity_regularizer=tf.keras.regularizers.l2(1e-7)
)(input_layer)
encoder = tf.keras.layers.Dropout(0.2)(encoder)
encoder = tf.keras.layers.Dense(hidden_dim_1, activation="relu")(encoder)
encoder = tf.keras.layers.Dense(hidden_dim_2, activation=tf.nn.leaky_relu)(encoder)

# Decoder
decoder = tf.keras.layers.Dense(hidden_dim_1, activation='relu')(encoder)
decoder = tf.keras.layers.Dropout(0.2)(decoder)
decoder = tf.keras.layers.Dense(encoding_dim, activation='relu')(decoder)
decoder = tf.keras.layers.Dense(input_dim, activation='tanh')(decoder)

# Autoencoder model
autoencoder = tf.keras.Model(inputs=input_layer, outputs=decoder)
autoencoder.summary()

# =========================
# Compile & Train
# =========================
autoencoder.compile(metrics=['accuracy'],
                    loss='mean_squared_error',
                    optimizer='adam')

history = autoencoder.fit(
    normal_train_data, normal_train_data,
    epochs=50,
    batch_size=64,
    shuffle=True,
    validation_data=(test_data, test_data),
    verbose=1
).history

# Plot training vs validation loss
plt.plot(history['loss'], label='Train')
plt.plot(history['val_loss'], label='Test')
plt.legend()
plt.title("Model Loss")
plt.show()

# =========================
# Evaluation
# =========================
test_predictions = autoencoder.predict(test_data)
mse = np.mean(np.power(test_data - test_predictions, 2), axis=1)
error_df = pd.DataFrame({'Reconstruction_error': mse, 'True_class': test_labels})

# Set threshold dynamically (95th percentile)
threshold = np.percentile(error_df.Reconstruction_error, 95)
print("Threshold (95th percentile):", threshold)

pred_y = [1 if e > threshold else 0 for e in error_df.Reconstruction_error.values]
error_df['pred'] = pred_y

# Confusion matrix
conf_matrix = confusion_matrix(error_df.True_class, error_df.pred)
sns.heatmap(conf_matrix,
            xticklabels=["Normal", "Fraud"],
            yticklabels=["Normal", "Fraud"], annot=True, fmt="d")
plt.title("Confusion Matrix")
plt.show()

# Metrics
print("Accuracy:", accuracy_score(error_df.True_class, error_df.pred))
print("Recall:", recall_score(error_df.True_class, error_df.pred))
print("Precision:", precision_score(error_df.True_class, error_df.pred))
