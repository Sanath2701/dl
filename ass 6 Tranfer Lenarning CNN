 !pip install tensorflow
 import tensorflow as tf
 from tensorflow.keras import layers, models
 from tensorflow.keras.preprocessing.image import ImageDataGenerator
 from tensorflow.keras.callbacks import EarlyStopping
 import matplotlib.pyplot as plt
 import numpy as np
 import seaborn as sns
 from sklearn.metrics import confusion_matrix, classification_report
 # Step 2: Dataset paths
 train_dir = train_dir = r"C:\Users\KIRAN\Downloads\archive\Small 
Object dataset\train" # Replace with your train folder
 val_dir = r"C:\Users\KIRAN\Downloads\archive\Small Object dataset\
 test"      # Replace with your validation/test folder
 # Step 3: Data preprocessing & strong augmentation
 train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=40,
    width_shift_range=0.3,
    height_shift_range=0.3,
    shear_range=0.3,
    zoom_range=0.3,
    horizontal_flip=True,
    fill_mode='nearest'
 )
 val_datagen = ImageDataGenerator(rescale=1./255)
 train_data = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=True
 )
 val_data = val_datagen.flow_from_directory(
    val_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=False
 )

base_model = tf.keras.applications.DenseNet121(
    input_shape=(224, 224, 3),
    include_top=False,
    weights='imagenet'
 )
 # Step 5: Freeze lower layers
 for layer in base_model.layers[:300]:
    layer.trainable = False
 # Step 6: Add custom classifier
 model = models.Sequential([
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.Dense(256, activation='relu', 
kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    layers.Dropout(0.5),
    layers.Dense(train_data.num_classes, activation='softmax')
 ])
 # Step 7: Compile model
 model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
 )
 # Step 8: Early stopping to prevent overfitting
 early_stop = EarlyStopping(
    monitor='val_loss', 
    patience=5, 
    restore_best_weights=True
 )
 # Step 9: Train classifier layers
 history = model.fit(
    train_data,
    epochs=5,
    validation_data=val_data,
    callbacks=[early_stop]
 )

 # Step 10: Fine-tuning top layers
 for layer in base_model.layers[300:]:
    layer.trainable = True
 model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
 )
 history_fine = model.fit(
    train_data,
    epochs=5,
    validation_data=val_data,
    callbacks=[early_stop]
 )
# Step 11: Evaluate model
 loss, acc = model.evaluate(val_data)
 print(f"Validation Accuracy: {acc*100:.2f}%")

# Step 12: Plot accuracy and loss curves
 plt.figure(figsize=(12,5))
 plt.subplot(1,2,1)
 plt.plot(history.history['accuracy'] + 
history_fine.history['accuracy'], label='Train Accuracy')
 plt.plot(history.history['val_accuracy'] + 
history_fine.history['val_accuracy'], label='Val Accuracy')
 plt.title("Accuracy over Epochs")
 plt.legend()
 plt.subplot(1,2,2)
 plt.plot(history.history['loss'] + history_fine.history['loss'], 
label='Train Loss')
 plt.plot(history.history['val_loss'] + 
history_fine.history['val_loss'], label='Val Loss')
 plt.title("Loss over Epochs")
 plt.legend()
 plt.show()

# Step 13: Confusion matrix and classification report
 val_data.reset()
 preds =
 model.predict(val_data)
 y_pred =
 np.argmax(preds, axis=1)
 y_true =
 val_data.classes
 class_names = 
list
 (val_data.class_indices.keys())
 # Confusion matrix
 cm =
 confusion_matrix(y_true, y_pred)
 plt.figure(figsize=(8,6
 ))
 sns.heatmap(cm, annot=
 True
 , fmt=
 yticklabels=
 class_names, cmap=
 plt.xlabel(
 "Predicted")
 plt.ylabel(
 "True")
 plt.title(
 "Confusion Matrix")
 plt.show()
 "d"
 , xticklabels=
 class_names, 
"Blues")
 # Precision, Recall, F1-score
 report =
 classification_report(y_true, y_pred, 
target_names=
 class_names)
 print(
 "Classification Report:
 \n"
 , report)
# Step 14: Predict on a single image
 from
 tensorflow.keras.preprocessing 
import
 image
 img_path = 
r"C:\Users\KIRAN\Downloads\archive\Small Object dataset\
 test\fly\img\fly002.jpg" 
# replace with your image path
img =
 image.load_img(img_path, target_size=(
 img_array =
 image.img_to_array(img) / 
255.0
 img_array =
 224, 
224))
 np.expand_dims(img_array, axis=0)
 prediction =
 model.predict(img_array)
 predicted_class =
 class_names[np.argmax(prediction)]
 print(
 "Predicted Object:"
 , predicted_class)
